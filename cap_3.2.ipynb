import numpy as np
import matplotlib.pyplot as plt

# Função auxiliar para desenhar a função 2D
def draw_2D_function(ax, x1_mesh, x2_mesh, y):
    pos = ax.contourf(x1_mesh, x2_mesh, y, levels=256 ,cmap='hot', vmin=-10, vmax=10.0)
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    levels = np.arange(-10, 10, 1.0)
    ax.contour(x1_mesh, x2_mesh, y, levels, cmap='winter')

# Função para plotar a rede neural rasa com 2 inputs e 1 output
def plot_neural_2_inputs(x1, x2, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3):
    fig, ax = plt.subplots(3, 3)
    fig.set_size_inches(8.5, 8.5)
    fig.tight_layout(pad=3.0)

    draw_2D_function(ax[0, 0], x1, x2, pre_1)
    ax[0, 0].set_title('Preactivation 1')

    draw_2D_function(ax[0, 1], x1, x2, pre_2)
    ax[0, 1].set_title('Preactivation 2')

    draw_2D_function(ax[0, 2], x1, x2, pre_3)
    ax[0, 2].set_title('Preactivation 3')

    draw_2D_function(ax[1, 0], x1, x2, act_1)
    ax[1, 0].set_title('Activation 1')

    draw_2D_function(ax[1, 1], x1, x2, act_2)
    ax[1, 1].set_title('Activation 2')

    draw_2D_function(ax[1, 2], x1, x2, act_3)
    ax[1, 2].set_title('Activation 3')

    draw_2D_function(ax[2, 0], x1, x2, w_act_1)
    ax[2, 0].set_title('Weighted Act 1')

    draw_2D_function(ax[2, 1], x1, x2, w_act_2)
    ax[2, 1].set_title('Weighted Act 2')

    draw_2D_function(ax[2, 2], x1, x2, w_act_3)
    ax[2, 2].set_title('Weighted Act 3')

    plt.show()

    # Plot da saída final da rede neural
    fig, ax = plt.subplots()
    draw_2D_function(ax, x1, x2, y)
    ax.set_title('Network Output')
    ax.set_aspect(1.0)
    plt.show()

# Função ReLU (Rectified Linear Unit)
def ReLU(preactivation):
    return np.maximum(0, preactivation)

# Função da rede neural rasa com 2 inputs, 1 output e 3 neurônios na camada oculta
def shallow_2_1_3(x1, x2, activation_fn, phi_0, phi_1, phi_2, phi_3,
                  theta_10, theta_11, theta_12, theta_20, theta_21, theta_22,
                  theta_30, theta_31, theta_32):

    # Calcula as pré-ativações da camada oculta
    pre_1 = theta_10 + theta_11 * x1 + theta_12 * x2
    pre_2 = theta_20 + theta_21 * x1 + theta_22 * x2
    pre_3 = theta_30 + theta_31 * x1 + theta_32 * x2

    # Aplica a função de ativação (ReLU)
    act_1 = activation_fn(pre_1)
    act_2 = activation_fn(pre_2)
    act_3 = activation_fn(pre_3)

    # Calcula as ativações ponderadas
    w_act_1 = phi_1 * act_1
    w_act_2 = phi_2 * act_2
    w_act_3 = phi_3 * act_3

    # Calcula a saída final
    y = phi_0 + w_act_1 + w_act_2 + w_act_3

    # Retorna todas as variáveis
    return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3

# Definição dos parâmetros
theta_10 = -4.0; theta_11 = 0.9; theta_12 = 0.0
theta_20 = 5.0; theta_21 = -0.9; theta_22 = -0.5
theta_30 = -7; theta_31 = 0.5; theta_32 = 0.9
phi_0 = 0.0; phi_1 = -2.0; phi_2 = 2.0; phi_3 = 1.5

# Definindo o grid de valores de entrada
x1 = np.arange(0.0, 10.0, 0.1)
x2 = np.arange(0.0, 10.0, 0.1)
x1, x2 = np.meshgrid(x1, x2)  # Cria o grid 2D para as entradas

# Executa a rede neural para todos os valores de entrada
y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \
    shallow_2_1_3(x1, x2, ReLU, phi_0, phi_1, phi_2, phi_3,
                  theta_10, theta_11, theta_12, theta_20, theta_21, theta_22,
                  theta_30, theta_31, theta_32)

# Plotando os resultados
plot_neural_2_inputs(x1, x2, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3)
