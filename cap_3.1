import numpy as np
import matplotlib.pyplot as plt

# Define a função ReLU
def ReLU(preactivation):
    # Aplica a função ReLU (máximo entre 0 e o valor)
    #Usar o clip
    return np.clip(preactivation, 0, None)

# Define a rede neural shallow com um input, um output e três unidades escondidas
def shallow_1_1_3(x, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):
    # Cálculo das pré-ativações (equações do final da figura 3.3a-c)
    pre_1 = theta_10 + theta_11 * x
    pre_2 = theta_20 + theta_21 * x
    pre_3 = theta_30 + theta_31 * x

    # Aplica a função de ativação ReLU
    act_1 = activation_fn(pre_1)
    act_2 = activation_fn(pre_2)
    act_3 = activation_fn(pre_3)

    # Pondera as ativações (equivalente à figura 3.3 g-i)
    w_act_1 = phi_1 * act_1
    w_act_2 = phi_2 * act_2
    w_act_3 = phi_3 * act_3

    # Combina as ativações ponderadas e adiciona phi_0 para obter a saída
    y = phi_0 + w_act_1 + w_act_2 + w_act_3

    return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3

# Função para plotar a rede neural
def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):
    # Plota os gráficos intermediários
    if plot_all:
        fig, ax = plt.subplots(3,3)
        fig.set_size_inches(8.5, 8.5)
        fig.tight_layout(pad=3.0)
        ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('Preactivation')
        ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('Preactivation')
        ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('Preactivation')
        ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('Activation')
        ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('Activation')
        ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('Activation')
        ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('Weighted Act')
        ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('Weighted Act')
        ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('Weighted Act')
        for plot_y in range(3):
            for plot_x in range(3):
                ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])
                ax[plot_y,plot_x].set_aspect(0.5)
        plt.show()

    fig, ax = plt.subplots()
    ax.plot(x,y)
    ax.set_xlabel('Input')
    ax.set_ylabel('Output')
    ax.set_xlim([0,1])
    ax.set_ylim([-1,1])
    ax.set_aspect(0.5)
    if x_data is not None:
        ax.plot(x_data, y_data, 'mo')
    plt.show()

# Função de perda de mínimos quadrados
def least_squares_loss(y_train, y_predict):
    # Calcula a soma dos erros quadrados
    return np.sum((y_train - y_predict)**2)

theta_10 =  0.2 ; theta_11 = -1.0
theta_20 = -1.0  ; theta_21 = 2.0
theta_30 = -0.5  ; theta_31 = 0.65
phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0

x = np.arange(0,1,0.01)

y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \
    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)

plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)

x_train = np.array([0.09291784, 0.46809093, 0.93089486, 0.67612654, 0.73441752, 0.86847339,
                    0.49873225, 0.51083168, 0.18343972, 0.99380898, 0.27840809, 0.38028817,
                    0.12055708, 0.56715537, 0.92005746, 0.77072270, 0.85278176, 0.05315950,
                    0.87168699, 0.58858043])
y_train = np.array([-0.15934537, 0.18195445, 0.45127015, 0.13921448, 0.09366691, 0.30567674,
                    0.37229117, 0.40716968, -0.08131792, 0.41187806, 0.36943738, 0.3994327,
                    0.01906257, 0.3582041, 0.45256496, -0.0183121, 0.02957665, -0.24354444,
                    0.14803884, 0.26824970])

y_predict, *_ = shallow_1_1_3(x_train, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)


loss = least_squares_loss(y_train, y_predict)
print('Your Loss = %3.3f, True value = 9.385' % loss)


